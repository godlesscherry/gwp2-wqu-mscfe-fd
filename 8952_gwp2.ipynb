{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Assessing Models with Alternative Data\n",
    "---\n",
    "## Q1. Data Understanding\n",
    "\n",
    "### Types of Data Used in the Research\n",
    "\n",
    "The study utilizes financial time series data from **Yahoo Finance** covering the period from **December 12, 2009, to January 1, 2020**. The raw data includes:\n",
    "\n",
    "- **Price-related features**: \n",
    "  - Opening price (Open)\n",
    "  - Highest price (High)\n",
    "  - Lowest price (Low)\n",
    "  - Closing price (Close)\n",
    "  - Adjusted closing price (Adjusted Close)\n",
    "- **Volume**: Number of transactions\n",
    "- **Fundamental events**: Dividends yield and capital gain distributions (through Adjusted Close)\n",
    "\n",
    "The authors focus on **three ETFs** representing different market dynamics:\n",
    "- iShares MSCI Chile ETF (**ECH**)\n",
    "- iShares MSCI Brazil ETF (**EWZ**)\n",
    "- iShares Core S&P 500 ETF (**IVV**)\n",
    "\n",
    "These datasets represent **emerging** and **developed** markets and are selected for cross-market comparison.\n",
    "\n",
    "\n",
    "### Technical Indicator Derivation\n",
    "\n",
    "To enrich the dataset, the authors computed **216 daily features** by applying the **Pandas TA (Technical Analysis)** library, a package designed to compute customizable financial indicators. These features fall into several categories:\n",
    "\n",
    "- **Trend indicators**: e.g., TTM Trend, Correlation Trend Indicator (CTI)\n",
    "- **Momentum indicators**: e.g., Williams %R (WILLR), Balance of Power (BOP)\n",
    "- **Cycle indicators**: e.g., Even Better SineWave (EBSW)\n",
    "- **Volume-based indicators**: e.g., Archer's On-Balance Volume (AOBV), Price Volume Rank (PVR)\n",
    "- **Volatility indicators**: e.g., Bollinger Band Percent (BBP)\n",
    "- **Statistical indicators**: e.g., Z-score\n",
    "- **Boolean delta indicators**: e.g., Increasing (INC), Decreasing (DEC)\n",
    "\n",
    "Each of these was computed using transformations of the six base price and volume attributes from Yahoo Finance. The formulae for selected indicators include:\n",
    "\n",
    "- **Balance of Power (BOP)**:\n",
    "  $$\n",
    "  \\text{BOP}_t = \\frac{c_t - o_t}{h_t - l_t}\n",
    "  $$\n",
    "\n",
    "- **Williams %R (WILLR)**:\n",
    "  $$\n",
    "  \\text{WILLR}_t = -100 \\cdot \\frac{\\text{Highest High} - c_t}{\\text{Highest High} - \\text{Lowest Low}}\n",
    "  $$\n",
    "\n",
    "- **Z-score (Zs)**:\n",
    "  $$\n",
    "  \\text{Zs}_t = \\frac{c_t - SMA_t}{\\sigma_t}\n",
    "  $$\n",
    "\n",
    "- **Decreasing (DEC)**:\n",
    "  $$\n",
    "  \\text{DEC}_t = \\begin{cases} 1, & \\text{if } c_t - c_{t-1} < 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Increasing (INC)**:\n",
    "  $$\n",
    "  \\text{INC}_t = \\begin{cases} 1, & \\text{if } c_t - c_{t-1} > 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Stochastic RSI (StochRSI)**:\n",
    "  $$\n",
    "  \\text{StochRSI}_t = \\frac{\\text{RSI}_t - \\min(\\text{RSI})}{\\max(\\text{RSI}) - \\min(\\text{RSI})}\n",
    "  $$\n",
    "\n",
    "- **Bollinger Band Percent (BBP)**:\n",
    "  $$\n",
    "  \\text{BBP}_t = \\frac{o_t - \\text{Lower Band}_t}{\\text{Upper Band}_t - \\text{Lower Band}_t}\n",
    "  $$\n",
    "\n",
    "- **On-Balance Volume (OBV)**:\n",
    "  $$\n",
    "  \\text{OBV}_t = \\begin{cases} \\text{OBV}_{t-1} + v_t, & \\text{if } c_t > c_{t-1} \\\\ \\text{OBV}_{t-1} - v_t, & \\text{if } c_t < c_{t-1} \\\\ \\text{OBV}_{t-1}, & \\text{otherwise} \\end{cases}\n",
    "  $$\n",
    "\n",
    "\n",
    "### Importance of Technical Indicators in Predicting Stock Movements\n",
    "\n",
    "- **Dimensional Enrichment**: The transformation of raw price and volume data into derived indicators enables the neural network to capture **nonlinear patterns** and **hidden cycles** in the market data.\n",
    "- **Feature Discrimination**: The study uses statistical methods (e.g., LASSO, Chi-squared, MAD, DR) to **rank and select features** that are most informative for classification. This ensures that only a small, optimized subset (about 5%) is used, thereby **reducing model complexity** without sacrificing accuracy.\n",
    "- **Performance Enhancement**: Selected indicators lead to significant improvements in classification accuracy (up to **13.63%**) and reductions in training time (about **84.68%**) as shown in Table 6 of the study.\n",
    "- **Market-Specific Insights**: The indicators reveal differences in market behavior. For example, emerging markets rely more on **cyclic and volume indicators** (e.g., AOBV, EBSW), while developed markets leverage **trend-following and momentum indicators** (e.g., TTM Trend, PVR).\n",
    "\n",
    "These findings highlight the **predictive power** of a curated subset of technical indicators and justify their role as key inputs in machine learning-based financial forecasting models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2. Security Understanding\n",
    "\n",
    "### Selected ETF: iShares MSCI Chile ETF (ECH)\n",
    "\n",
    "### Asset Type\n",
    "\n",
    "- **Fund Name**: iShares MSCI Chile ETF\n",
    "- **Ticker**: ECH\n",
    "- **Asset Class**: Equity\n",
    "- **Type**: Exchange-Traded Fund (ETF)\n",
    "- **Region Focus**: Chile (Emerging Market)\n",
    "- **Issuer**: BlackRock\n",
    "- **Benchmark Index**: MSCI Chile IMI 25/50 Index\n",
    "- **Expense Ratio**: ~0.57% (typical range as per ETF factsheets)\n",
    "- **Inception Date**: November 20, 2007\n",
    "\n",
    "This ETF offers exposure to large-, mid-, and small-cap Chilean equities. As a single-country ETF, it is relatively more volatile but suitable for those seeking concentrated exposure to Latin America's mining and utilities-heavy economy.\n",
    "\n",
    "\n",
    "### Historical Price Chart (2009–2020)\n",
    "\n",
    "*The price chart referred in the paper (Figure 2) plots daily opening prices of ECH from 2009 to 2020. The prices ranged roughly between \\$29.30 and \\$80.25.*\n",
    "\n",
    "| Statistic        | Value (USD) |\n",
    "|------------------|--------------|\n",
    "| Minimum          | 29.30        |\n",
    "| 1st Quartile     | 40.35        |\n",
    "| Median           | 46.48        |\n",
    "| Mean             | 50.10        |\n",
    "| 3rd Quartile     | 59.84        |\n",
    "| Maximum          | 80.25        |\n",
    "\n",
    "\n",
    "\n",
    "### Key Statistics and Sector Breakdown\n",
    "\n",
    "As per Table 1 in the paper, the top sector exposures for ECH are:\n",
    "\n",
    "| Sector          | Weight (%) |\n",
    "|------------------|------------|\n",
    "| Financials       | 21.53      |\n",
    "| Materials        | 21.28      |\n",
    "| Utilities        | 18.73      |\n",
    "| Consumer Staples| 13.92      |\n",
    "| Energy           | 8.34       |\n",
    "| **Total**        | **83.8**   |\n",
    "\n",
    "This reflects Chile's dependence on **mining, utilities, and financials**, especially copper exports and public infrastructure.\n",
    "\n",
    "\n",
    "\n",
    "### Why Classification Over Regression?\n",
    "\n",
    "The paper uses a **classification model** to predict the **directional movement** of the ETF's price (up or down), rather than forecasting the exact price level.\n",
    "\n",
    "- **Response Variable Definition**:\n",
    "  $$\n",
    "  \\Gamma_t = \\begin{cases}\n",
    "    1, & \\text{if } Open_t - Open_{t-1} > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Justification**:\n",
    "  - Classification is less sensitive to prediction errors than regression.\n",
    "  - Trend direction is more actionable for traders/investors.\n",
    "  - Emerging market data is noisier, making regression less stable.\n",
    "  - Easier to calibrate trading strategies on binary outcomes (e.g., long vs. short).\n",
    "\n",
    "\n",
    "### Alternative Label Definitions for Classification\n",
    "\n",
    "1. **Volatility-Adjusted Trend Class**:\n",
    "   $$\n",
    "   \\Gamma_t = \\begin{cases}\n",
    "     1, & \\text{if } (Open_t - Open_{t-1}) > \\theta \\cdot \\sigma_t \\\\\n",
    "     0, & \\text{otherwise}\n",
    "   \\end{cases}\n",
    "   $$\n",
    "   - Where $\\sigma_t$ is rolling volatility and $\\theta$ is a chosen threshold.\n",
    "   - Captures significant moves only, avoiding noise.\n",
    "\n",
    "2. **Return-Based 3-Class Label**:\n",
    "   $$\n",
    "   \\Gamma_t = \\begin{cases}\n",
    "     1, & \\text{if return} > \\epsilon \\\\\n",
    "     0, & \\text{if } |return| \\leq \\epsilon \\\\\n",
    "    -1, & \\text{if return} < -\\epsilon\n",
    "   \\end{cases}\n",
    "   $$\n",
    "   - A trinary classification capturing uptrend, no change, and downtrend.\n",
    "   - Enables more nuanced decision models.\n",
    "\n",
    "These alternative labels could improve model interpretability and control for market microstructure noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q3. Methodology Understanding\n",
    "\n",
    "### Reorganized \"Data\" Section\n",
    "\n",
    "#### 2.1 Data Collection\n",
    "\n",
    "The authors used historical financial data from **Yahoo Finance** for three ETFs—ECH, EWZ, and IVV—spanning the period from **December 12, 2009, to January 1, 2020**. The raw data includes:\n",
    "\n",
    "- Open, High, Low, Close, Adjusted Close prices\n",
    "- Trading volume\n",
    "\n",
    "This data was collected to reflect pre-pandemic market behavior, avoiding distortions caused by COVID-19.\n",
    "\n",
    "#### 2.2 Preprocessing\n",
    "\n",
    "- **Data Cleaning**: Rows with missing values (e.g., due to indicators like SMA requiring a lookback period) were dropped.\n",
    "- **Normalization**: A min-max transformation was applied to each feature:\n",
    "\n",
    "  $$\n",
    "  x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "  $$\n",
    "\n",
    "- **Class Assignment**: The response variable $\\Gamma_t$ was created for binary classification:\n",
    "\n",
    "  $$\n",
    "  \\Gamma_t = \\begin{cases}\n",
    "    1, & \\text{if } Open_t - Open_{t-1} > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "#### 2.3 Technical Indicator Construction\n",
    "\n",
    "Using the **Pandas TA** library, over **210 indicators** were computed from base price and volume data. These indicators fall into categories such as:\n",
    "\n",
    "- Trend (e.g., TTM Trend)\n",
    "- Momentum (e.g., Balance of Power, Williams %R)\n",
    "- Volume (e.g., OBV, PVR)\n",
    "- Volatility (e.g., Bollinger Band %)\n",
    "- Cycles and Statistics (e.g., EBSW, Z-score)\n",
    "\n",
    "The final feature set included **216 technical indicators**.\n",
    "\n",
    "\n",
    "### Renamed Section 3: Methodology\n",
    "\n",
    "#### 3.1 Descriptive Statistics: Pearson Correlation\n",
    "\n",
    "Pearson correlation was used to evaluate linear relationships between features:\n",
    "\n",
    "$$\n",
    "r_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n \\cdot \\sigma_x \\sigma_y}\n",
    "$$\n",
    "\n",
    "This guided initial filtering of irrelevant features.\n",
    "\n",
    "#### 3.2 Predictive Feature Selection Techniques\n",
    "\n",
    "To reduce dimensionality, the following methods were applied:\n",
    "\n",
    "- **Low Variance Filter**\n",
    "- **Chi-Squared Test**\n",
    "- **LASSO (Least Absolute Shrinkage and Selection Operator)**:\n",
    "\n",
    "  $$\n",
    "  \\text{LASSO Loss} = \\text{RSS} + \\lambda \\sum |w_j|\n",
    "  $$\n",
    "\n",
    "- **Tree-Based Models (Extra Trees Classifier)**\n",
    "- **Principal Feature Analysis (PFA)**\n",
    "- **Mean Absolute Difference (MAD)**\n",
    "- **Dispersion Ratio (DR)**\n",
    "\n",
    "Each method selected the most informative features, and the top quartile of features from each method were aggregated to form subsets.\n",
    "\n",
    "#### 3.3 Model Training: Neural Network (MLP)\n",
    "\n",
    "A **Multilayer Perceptron (MLP)** was trained using the selected features. The MLP architecture:\n",
    "\n",
    "- Hidden Layer Size: $(\\text{input features} + \\text{output classes}) / 2$\n",
    "- Activation: logistic\n",
    "- Solver: L-BFGS\n",
    "- Learning Rate: adaptive (initial = 0.03)\n",
    "- Max Iterations: 5000\n",
    "\n",
    "#### 3.4 Cross-Validation\n",
    "\n",
    "A **10-fold cross-validation** strategy was applied:\n",
    "\n",
    "- Split dataset into 10 partitions\n",
    "- Rotate each as the test set, training on the remaining 9\n",
    "- Average results for final accuracy metric\n",
    "\n",
    "\n",
    "\n",
    "### Indicator Optimization Strategy\n",
    "\n",
    "The feature selection process was iteratively refined:\n",
    "\n",
    "1. Each ETF underwent separate preprocessing and feature selection.\n",
    "2. Features present in at least **n = 5** statistical subsets were selected.\n",
    "3. MLPs were trained on both full and reduced feature sets.\n",
    "4. **Selected(5)** subset (~10 features) achieved:\n",
    "   - Up to **13.63%** gain in accuracy\n",
    "   - Over **84%** reduction in training time\n",
    "\n",
    "This confirms that optimized technical indicators improve neural network performance by:\n",
    "- Reducing overfitting\n",
    "- Lowering computational load\n",
    "- Improving generalization to unseen data\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Feature Understanding\n",
    "\n",
    "### What Constitutes a Feature in the Study\n",
    "\n",
    "In this study, a **feature** is a derived quantitative variable that characterizes some aspect of the price or volume behavior of an ETF. Examples include:\n",
    "\n",
    "- **Raw features**: Open, High, Low, Close, Adjusted Close, Volume\n",
    "- **Derived technical indicators**: Computed using formulas applied to raw features (e.g., Bollinger Band %, Balance of Power, Z-score)\n",
    "\n",
    "Each feature is represented numerically and updated daily, forming a multivariate time series used as input for machine learning models.\n",
    "\n",
    "\n",
    "### Difference Between Features, Methods, and Models\n",
    "\n",
    "- **Features**: Independent variables (e.g., BBP, OBV) used as inputs for prediction\n",
    "- **Methods**: Statistical or machine learning techniques used to select or transform features (e.g., LASSO, Chi-squared, PCA)\n",
    "- **Models**: Algorithms that learn from features to predict the target variable (e.g., Multilayer Perceptron)\n",
    "\n",
    "| Category  | Examples                             |\n",
    "|-----------|--------------------------------------|\n",
    "| Features  | WILLR, BBP, J, CTI, AOBV              |\n",
    "| Methods   | Pearson correlation, MAD, LASSO       |\n",
    "| Models    | MLP, Tree-based classifiers           |\n",
    "\n",
    "\n",
    "\n",
    "### Categorization of Features Used\n",
    "\n",
    "The 216 derived features are categorized as follows (based on Pandas TA):\n",
    "\n",
    "| Feature Category | Examples                    | Selected in Final Subset? |\n",
    "|------------------|-----------------------------|----------------------------|\n",
    "| **Trend**        | TTM_TRND, CTI               | Yes                        |\n",
    "| **Momentum**     | WILLR, BOP, KDJ             | Yes                        |\n",
    "| **Volume**       | OBV, PVR, AOBV              | Yes                        |\n",
    "| **Volatility**   | BBP                         | Yes                        |\n",
    "| **Cycles**       | Even Better SineWave (EBSW) | Yes                        |\n",
    "| **Statistics**   | Z-score                     | Yes                        |\n",
    "| **Boolean**      | INC, DEC                    | Yes                        |\n",
    "\n",
    "These features were engineered from historical price and volume series to capture specific trading patterns, cycle behaviors, or regime shifts.\n",
    "\n",
    "\n",
    "\n",
    "### Optimization Strategy and Its Role in Neural Network Performance\n",
    "\n",
    "The study adopts a **multi-metric feature selection pipeline**, which:\n",
    "\n",
    "1. Applies statistical filters (e.g., LASSO, Chi-Squared, Pearson) to rank all 216 features.\n",
    "2. Retains features appearing in at least **5 of the individual top-25% lists**, creating the `Selected(5)` subset.\n",
    "3. Trains the MLP using only these reduced features.\n",
    "\n",
    "**Why Optimization Improves Performance**:\n",
    "\n",
    "- **Avoids Overfitting**: Irrelevant or redundant features introduce noise.\n",
    "- **Speeds Training**: Smaller input space means faster convergence.\n",
    "- **Increases Accuracy**: By focusing on the most informative signals, the model generalizes better.\n",
    "\n",
    "In summary, feature optimization acts as a filter that amplifies meaningful patterns while suppressing noise, making it essential for improving neural network performance in high-dimensional financial forecasting tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Optimization Understanding\n",
    "\n",
    "### Concept of Cross-Validation\n",
    "\n",
    "**Cross-validation** is a resampling method used to evaluate the generalizability of a predictive model. It splits the dataset into multiple subsets (folds), trains the model on some, and tests it on the remaining ones. This helps estimate how the model performs on unseen data, reducing both **overfitting** and **underfitting**.\n",
    "\n",
    "\n",
    "### How k-Fold Cross-Validation Works\n",
    "\n",
    "In **k-fold cross-validation**:\n",
    "\n",
    "1. The dataset is split into $k$ equally sized folds.\n",
    "2. The model is trained $k$ times, each time:\n",
    "   - Using $k-1$ folds for training\n",
    "   - Using the remaining 1 fold for testing\n",
    "3. The process rotates the test fold through all $k$ partitions.\n",
    "4. The final performance metric is the average across all $k$ trials.\n",
    "\n",
    "For this study, the authors used **10-fold cross-validation** ($k = 10$), which provides a balance between bias and variance in error estimation.\n",
    "\n",
    "\n",
    "### Jaccard Distance and Comparison to Other Metrics\n",
    "\n",
    "The **Jaccard distance** measures dissimilarity between two sets $A$ and $B$:\n",
    "\n",
    "$$\n",
    "J(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "- **Range**: [0, 1]\n",
    "  - 0: identical sets\n",
    "  - 1: no overlap\n",
    "\n",
    "#### Compared With:\n",
    "\n",
    "1. **Euclidean Distance**:\n",
    "   $$\n",
    "   d(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}\n",
    "   $$\n",
    "   - Measures absolute geometric distance.\n",
    "   - Sensitive to scale and magnitude.\n",
    "\n",
    "2. **Manhattan Distance**:\n",
    "   $$\n",
    "   d(x, y) = \\sum_i |x_i - y_i|\n",
    "   $$\n",
    "   - Measures city-block (grid-based) distance.\n",
    "   - Less sensitive to outliers than Euclidean.\n",
    "\n",
    "**Jaccard** is preferred for **set similarity**, especially when working with **binary or categorical feature sets** like those selected by statistical methods.\n",
    "\n",
    "### Optimality Definition and Evaluation in the Paper\n",
    "\n",
    "**Optimality** in this paper refers to the ability to select a **minimal set of features** that achieves **maximum classification accuracy** with **minimal computational cost**.\n",
    "\n",
    "- The authors define **Selected(n)** as the set of features appearing in at least `n` statistical selection methods.\n",
    "- For $n = 5$, the `Selected(5)` subset:\n",
    "  - Contains only 9–10 features (≈5% of total)\n",
    "  - Achieves near or better performance than full 216-feature set\n",
    "\n",
    "### Evaluation Criteria:\n",
    "\n",
    "- **Classification Accuracy** from 10-fold cross-validation\n",
    "- **Training Time Reduction**\n",
    "- **Jaccard Similarity** among ETFs (to compare robustness of selected features)\n",
    "\n",
    "The result: **80%+ accuracy** with only **5% of the features**, confirming the optimality of the selection strategy.\n",
    "\n",
    "**Conclusion**:\n",
    "Optimization was achieved via feature pruning, validation loops, and dissimilarity metrics—balancing performance with parsimony.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Financial Problem\n",
    "\n",
    "### Financial Problem Being Addressed\n",
    "\n",
    "The study addresses the problem of **predicting daily trend direction (up or down) in the price of Exchange-Traded Funds (ETFs)** using optimized sets of technical indicators. Specifically, the authors focus on ETFs in emerging markets (ECH and EWZ) and compare their behavior with a developed market ETF (IVV).\n",
    "\n",
    "- **Objective**: Improve the predictive performance and computational efficiency of machine learning models—particularly neural networks—by systematically selecting the most informative technical indicators.\n",
    "\n",
    "- **Modeling Strategy**: Treat this as a **binary classification problem** using a Multilayer Perceptron (MLP), where the model forecasts whether the opening price will be higher or lower than the previous day.\n",
    "\n",
    "- **Input Data**: 216 technical indicators derived from raw market data.\n",
    "\n",
    "- **Response Variable**:\n",
    "  $$\n",
    "  \\Gamma_t = \\begin{cases}\n",
    "    1, & \\text{if } Open_t - Open_{t-1} > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "### Influence of Emerging Market Dynamics on Model Design\n",
    "\n",
    "Emerging markets like Chile and Brazil are:\n",
    "- More volatile and less liquid\n",
    "- Influenced by political risk, commodity prices, and exchange rate shocks\n",
    "\n",
    "These characteristics introduce challenges:\n",
    "- **Higher noise-to-signal ratio**: Makes regression-based forecasting less reliable.\n",
    "- **Nonlinear, chaotic dynamics**: Favors models that can capture nonlinearities (e.g., neural networks).\n",
    "- **Cyclic behavior**: Markets may respond to cyclical macroeconomic or political events.\n",
    "\n",
    "**Design Adaptations**:\n",
    "- Use of classification instead of regression\n",
    "- Inclusion of **cycle-sensitive indicators** (e.g., EBSW)\n",
    "- Data normalization and cleaning to manage outliers\n",
    "- Emphasis on **feature selection** to isolate the most robust predictors under volatility\n",
    "\n",
    "In contrast, developed markets like the U.S. (IVV) exhibit:\n",
    "- Greater stability and liquidity\n",
    "- Broader sectoral diversification\n",
    "\n",
    "Thus, fewer indicators are needed, and **trend-following indicators** (e.g., TTM Trend, PVR) perform better. The study finds that the optimal feature sets vary meaningfully between emerging and developed markets.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Application\n",
    "\n",
    "### Main Result Takeaways\n",
    "\n",
    "1. **Feature Optimization Pays Off**:\n",
    "   - The `Selected(5)` subset, comprising ~10 features (≈5% of the full set), achieved **equal or better accuracy** compared to the full 216-feature set.\n",
    "   - Average improvement in accuracy: **up to 13.63%**\n",
    "   - Average reduction in training time: **84.68%**\n",
    "\n",
    "2. **Binary Classification Is Effective**:\n",
    "   - The model successfully predicts trend direction (up or down) using just technical indicators derived from daily market data.\n",
    "\n",
    "3. **Robust Cross-Validation**:\n",
    "   - Results were validated using **10-fold cross-validation**, enhancing the reliability of performance estimates.\n",
    "\n",
    "4. **Emerging vs. Developed Market Differences**:\n",
    "   - Emerging market ETFs (ECH, EWZ) tend to benefit more from **cycle-sensitive** and **volume-based** features.\n",
    "   - Developed market ETF (IVV) performs better with **trend-following** and **momentum** indicators.\n",
    "\n",
    "5. **Model Used**: Multilayer Perceptron (MLP) with logistic activation and adaptive learning, fine-tuned for binary output.\n",
    "\n",
    "\n",
    "### Most Useful Features Identified\n",
    "\n",
    "The top features selected in the `Selected(5)` subset include:\n",
    "\n",
    "| Feature         | Category     | Description                                      |\n",
    "|----------------|--------------|--------------------------------------------------|\n",
    "| BBP_5_2.0       | Volatility    | Bollinger Band Percent                          |\n",
    "| BOP             | Momentum     | Balance of Power                                |\n",
    "| DEC_1           | Boolean       | Decreasing close (boolean delta)                |\n",
    "| INC_1           | Boolean       | Increasing close (boolean delta)                |\n",
    "| J_9_3           | Momentum     | KDJ random index                                |\n",
    "| AOBV_LR_2       | Volume        | Archer's On Balance Volume                      |\n",
    "| CTI_12          | Trend         | Correlation Trend Indicator                     |\n",
    "| EBSW_40_10      | Cycles        | Even Better SineWave                            |\n",
    "| STOCHRSIk_14... | Momentum     | Stochastic RSI                                  |\n",
    "| TTM_TRND_6      | Trend         | Trailing Twelve Month Trend (IVV only)          |\n",
    "\n",
    "These features:\n",
    "- Capture price dynamics (INC, DEC)\n",
    "- Detect market pressure (BOP, AOBV)\n",
    "- Identify cyclical turning points (EBSW, KDJ)\n",
    "- Track volatility-adjusted price positions (BBP)\n",
    "\n",
    "Together, they form a **parsimonious yet highly informative feature set** that drives the model’s success.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Replication (Mini Lab)\n",
    "\n",
    "### ETF Chosen: iShares MSCI Chile ETF (ECH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display\n",
    "\n",
    "# Downloading historical data\n",
    "ech = yf.download(\"ECH\", start=\"2009-12-12\", end=\"2020-01-01\")\n",
    "ech = ech[['Open']].dropna()\n",
    "\n",
    "# Creating the response variable Γ_t\n",
    "ech['Gamma'] = (ech['Open'].diff() > 0).astype(int)\n",
    "ech = ech.dropna()\n",
    "\n",
    "# Example metric: Dispersion (Standard Deviation)\n",
    "ech['Dispersion'] = ech['Open'].rolling(window=14).std()\n",
    "ech = ech.dropna()\n",
    "\n",
    "# Normalize features\n",
    "ech['Open_norm'] = (ech['Open'] - ech['Open'].min()) / (ech['Open'].max() - ech['Open'].min())\n",
    "ech['Dispersion_norm'] = (ech['Dispersion'] - ech['Dispersion'].min()) / (ech['Dispersion'].max() - ech['Dispersion'].min())\n",
    "\n",
    "X = ech[['Open_norm', 'Dispersion_norm']].values\n",
    "y = ech['Gamma'].values\n",
    "\n",
    "# k-Fold Cross-Validation\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "acc_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', solver='lbfgs', max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print raw scores\n",
    "print(\"Cross-Validated Accuracy Scores:\", acc_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(acc_scores))\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame({\n",
    "    'Model Variant': ['Simplified MLP Model'],\n",
    "    'Features Used': ['Open + Dispersion'],\n",
    "    'Accuracy (%)': [round(np.mean(acc_scores) * 100, 2)],\n",
    "    'Notes': ['Binary classification using normalized features, 10-fold CV']\n",
    "})\n",
    "\n",
    "print(\"\\nReproduced Accuracy Table:\")\n",
    "display(results_df)\n",
    "\n",
    "# Plotting the Open Price and Dispersion\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(ech.index, ech['Open'], label='Open Price')\n",
    "plt.plot(ech.index, ech['Dispersion'], label='14-day Dispersion')\n",
    "plt.title('ECH Open Price vs. Dispersion')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page 1: Introduction and Data Sources – Web Scraping in Financial Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the modern era of digital finance, **alternative data** has gained prominence as an indispensable tool for investors, analysts, and researchers seeking insights beyond traditional financial statements and market data. Among the most accessible and powerful forms of alternative data is **web scraping**, a method for systematically extracting unstructured data from online sources. This page sets the foundation for understanding how web scraping is used in financial contexts, particularly for deriving investment signals, identifying trends, and improving the accuracy of predictive models.\n",
    "\n",
    "The digital footprint left by companies, financial institutions, consumers, and the media is vast and constantly evolving. This footprint, if mined effectively, can reveal market sentiment, product demand, regulatory risks, and competitive positioning. As structured datasets from traditional vendors become commoditized, firms now seek alpha-generating signals in less conventional places—web forums, news portals, regulatory filings, and social media.\n",
    "\n",
    "## Role of Web Scraping in Finance\n",
    "\n",
    "**Web scraping** involves automating the collection of publicly accessible data from websites using tools and scripts. It allows for near real-time data harvesting from a wide array of sources, ranging from breaking news headlines to customer reviews. The main advantages include:\n",
    "\n",
    "- **Speed**: Automated scripts can collect data at frequencies far higher than manual methods.\n",
    "- **Coverage**: Scraping allows data collection across diverse domains and geographies.\n",
    "- **Customizability**: Scrapers can be tuned to extract specific content (e.g., earnings dates, CEO quotes, job postings).\n",
    "\n",
    "Financial institutions deploy web scraping to support **quantitative trading, risk monitoring, and portfolio construction**. For instance, hedge funds scrape Twitter to gauge public sentiment on stocks, while equity analysts scrape company investor relations pages for press releases before they reach terminals.\n",
    "\n",
    "## Examples of Financial Web Scraping Use Cases\n",
    "\n",
    "1. **Earnings Announcements**: Scraping the IR (Investor Relations) sections of public company websites to detect earnings releases or call transcripts.\n",
    "2. **Regulatory Disclosures**: Downloading filings such as 10-Ks, 10-Qs, and 8-Ks from the SEC’s EDGAR database.\n",
    "3. **News Monitoring**: Real-time scraping of news headlines for market-moving developments.\n",
    "4. **Consumer Sentiment**: Mining product reviews or social media commentary to assess demand for a company’s products.\n",
    "5. **Job Postings**: Tracking hiring trends through company careers pages and LinkedIn to assess expansion.\n",
    "\n",
    "## Key Sources of Scraped Financial Data\n",
    "\n",
    "| Source Type        | Examples                                                | Potential Insight                             |\n",
    "|--------------------|---------------------------------------------------------|------------------------------------------------|\n",
    "| **News Media**     | CNBC, Bloomberg, Financial Times                        | Event-driven volatility, earnings reports     |\n",
    "| **Social Media**   | Twitter, Reddit, StockTwits                             | Retail sentiment, buzz analysis               |\n",
    "| **Official Filings**| SEC EDGAR, Companies House                             | Legal disclosures, executive turnover         |\n",
    "| **Forums & Blogs** | Seeking Alpha, Yahoo Finance                           | Analyst opinions, rumors                      |\n",
    "| **Corporate Sites**| Press releases, financial calendars                     | Forecast changes, product launches            |\n",
    "| **E-commerce**     | Amazon reviews, eBay listings                           | Demand proxies, pricing trends                |\n",
    "\n",
    "## Importance for Alpha Generation\n",
    "\n",
    "Alternative datasets scraped from the web are prized not only for their informational value but also for their timeliness. While traditional financial data is backward-looking and updated quarterly, web data reflects current market activity and sentiment. If properly processed, this can lead to:\n",
    "\n",
    "- **Early signal detection**: Identifying trends before they are priced in\n",
    "- **Behavioral insights**: Understanding irrational market reactions through social chatter\n",
    "- **Sentiment overlay**: Enhancing models by including soft information alongside hard metrics\n",
    "\n",
    "In sum, web scraping serves as a bridge between the deluge of public digital information and actionable investment insights. However, this power also comes with ethical and legal constraints, which will be explored in the next section.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Page 2 will delve into the different types of data obtained through web scraping and the quality concerns associated with them.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page 2: Types of Data and Quality Concerns in Web Scraping\n",
    "\n",
    "## Overview\n",
    "\n",
    "In the financial domain, web scraping serves as a powerful tool to gather alternative datasets that are not available through traditional financial data providers. However, the utility of scraped data depends heavily on its **type**, **granularity**, and **quality**. On this page, we explore the major categories of data collected through web scraping and highlight the primary quality-related challenges that must be addressed to ensure reliability and consistency in financial modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Types of Data Scraped from the Web\n",
    "\n",
    "Web scraping can extract multiple formats and structures of data, often unstructured in nature. The most relevant data types for finance include:\n",
    "\n",
    "### a. Textual Data\n",
    "- **News Headlines**: Often scraped from financial news portals such as Bloomberg, CNBC, and Reuters.\n",
    "- **Articles & Blogs**: Longer narratives from forums (e.g., Seeking Alpha) or economic commentary blogs.\n",
    "- **Filings & Disclosures**: Legal documents and investor updates from sources like EDGAR or company websites.\n",
    "- **Company Descriptions**: Extracted from public profile pages, used in fundamental analysis.\n",
    "\n",
    "### b. Sentiment Indicators\n",
    "- **Social Media Posts**: Tweets, Reddit threads, StockTwits messages—valuable for gauging investor mood.\n",
    "- **Review Scores**: Star ratings from e-commerce or app platforms as a proxy for product performance.\n",
    "- **Opinion Mining**: NLP-based parsing of subjective statements (e.g., bullish/bearish tone).\n",
    "\n",
    "### c. Event Data\n",
    "- **Product Launch Announcements**\n",
    "- **Executive Changes or Layoffs**\n",
    "- **M&A Rumors**\n",
    "- **Earnings Calls Schedules**\n",
    "\n",
    "### d. Count-Based Metrics\n",
    "- **Mention Frequency**: How often a company or ticker is discussed online.\n",
    "- **Upvotes or Likes**: Proxy for virality or popularity of a financial opinion.\n",
    "- **Comment Volume**: Especially useful on earnings threads or stock forums.\n",
    "\n",
    "### e. Commercial Metrics\n",
    "- **Product Prices and Discounts**: Scraped from Amazon, Walmart, etc.\n",
    "- **Availability & Stock-outs**: Indicates supply chain disruptions.\n",
    "- **Competitor Monitoring**: Product rollouts and pricing strategy.\n",
    "\n",
    "These data types are commonly used to build **nowcasting models**, **news-based volatility filters**, and **event-based alpha factors** in hedge funds and quant trading shops.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Quality Challenges in Web Scraping\n",
    "\n",
    "Although the breadth of data available through web scraping is impressive, it introduces several **quality-related issues** that must be managed carefully before applying the data in predictive or trading models:\n",
    "\n",
    "### a. Noise and Redundancy\n",
    "- Duplicate content across news aggregators or reposted tweets can inflate signal strength.\n",
    "- Scraped HTML often contains boilerplate material, ads, or irrelevant metadata.\n",
    "\n",
    "### b. Inconsistent Formatting\n",
    "- Web structures change frequently, which can break scrapers.\n",
    "- Lack of standardized APIs leads to irregular data fields and encoding issues.\n",
    "\n",
    "### c. Missing or Incomplete Data\n",
    "- Headlines may be truncated.\n",
    "- Dates or authors may be missing.\n",
    "- Social media posts may be deleted or shadowbanned.\n",
    "\n",
    "### d. Temporal Misalignment\n",
    "- Data timestamp may reflect extraction time, not publication time.\n",
    "- Real-time applications require proper synchronization across data sources.\n",
    "\n",
    "### e. Source Reliability and Bias\n",
    "- Forums like Reddit or Twitter are prone to **herding behavior**, **pump-and-dump schemes**, and **bot manipulation**.\n",
    "- News sites may have **political or editorial biases** that affect the framing of information.\n",
    "\n",
    "### f. Legal and Compliance Risks\n",
    "- Some data is behind login walls or terms-of-service restrictions.\n",
    "- Web scraping without adherence to `robots.txt` or fair use principles may be in legal grey areas.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Addressing Data Quality in Practice\n",
    "\n",
    "To mitigate these issues, practitioners typically:\n",
    "- **Apply cleaning pipelines**: Removing HTML tags, whitespace, duplicates, and invalid characters.\n",
    "- **Normalize entities**: Mapping synonyms and ticker aliases to canonical names.\n",
    "- **Use validation sets**: Benchmarking scraped sentiment or mentions against actual market reactions.\n",
    "- **Monitor scraper health**: Automated alerting when scraper fails or output quality declines.\n",
    "\n",
    "Robust preprocessing is often the difference between noise and insight. The goal is to distill useful, investment-grade signals from the raw, chaotic web.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Page 3 will examine the ethical and legal considerations around scraping and present example Python code to structure collected data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page 3: Ethical Considerations and Python Code for Web Scraping\n",
    "\n",
    "## 1. Ethical and Legal Concerns in Web Scraping\n",
    "\n",
    "Web scraping exists in a **legally ambiguous** and ethically nuanced space. While scraping publicly available information may seem harmless, it can cross ethical boundaries or even legal ones depending on context, source, and intent. Financial professionals, especially those operating in regulated environments, must carefully navigate this domain.\n",
    "\n",
    "### a. Terms of Service (ToS) Violations\n",
    "- Many websites explicitly disallow scraping in their ToS.\n",
    "- Ignoring these restrictions can lead to legal consequences, including cease-and-desist letters or lawsuits.\n",
    "- High-profile cases like *hiQ Labs vs. LinkedIn* have shown that even scraping publicly visible data can result in litigation.\n",
    "\n",
    "### b. Server Load and Resource Abuse\n",
    "- Aggressive scraping scripts can **overwhelm servers**, disrupting the host site.\n",
    "- Ethical scraping includes **rate limiting**, **respecting `robots.txt`**, and **avoiding denial-of-service behavior**.\n",
    "\n",
    "### c. Data Ownership and Copyright\n",
    "- Scraped content (e.g., analyst reports or editorial articles) may be protected by copyright law.\n",
    "- Republishing such data or using it for commercial gain without attribution or license may constitute infringement.\n",
    "\n",
    "### d. Privacy Risks\n",
    "- Scraping forums or social media can inadvertently collect **personally identifiable information (PII)**.\n",
    "- Examples include usernames, geolocations, or health-related disclosures.\n",
    "- Responsible data use demands **de-identification**, **anonymization**, and **compliance with regulations like GDPR**.\n",
    "\n",
    "### e. Reputational and Compliance Risk\n",
    "- Financial firms deploying scrapers must consider internal **risk policies**, **compliance audits**, and **client-facing disclosures**.\n",
    "- Regulatory bodies may require documentation of data provenance and assurance that models are not based on unauthorized data.\n",
    "\n",
    "\n",
    "## 2. Best Practices for Ethical Web Scraping\n",
    "\n",
    "To conduct ethical web scraping in a financial context:\n",
    "\n",
    "- Use **public APIs** when possible (e.g., Twitter API, SEC’s EDGAR FTP interface).\n",
    "- Always **check and respect `robots.txt`** files to see what is allowed.\n",
    "- **Throttle requests** (e.g., 1 request/second) to avoid overloading servers.\n",
    "- **Log access patterns** and **implement exponential backoff** on errors.\n",
    "- Avoid **scraping paywalled or account-protected content** unless permission is obtained.\n",
    "- Maintain a record of **source licenses, ToS screenshots, and scraping logs**.\n",
    "\n",
    "Adhering to these practices reduces risk while preserving the integrity of the data science process.\n",
    "\n",
    "\n",
    "## 3. Sample Python Code for Basic Scraping and Structuring\n",
    "\n",
    "Below is a simplified example using `requests` and `BeautifulSoup` to scrape and structure headlines from a financial news website (e.g., CNBC):\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define the target URL\n",
    "url = \"https://www.cnbc.com/finance/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract headline elements (adjust based on site structure)\n",
    "data = []\n",
    "for article in soup.find_all('a', class_='Card-title'):\n",
    "    headline = article.get_text(strip=True)\n",
    "    link = article.get('href')\n",
    "    data.append({\"headline\": headline, \"url\": link})\n",
    "\n",
    "# Store in a structured format\n",
    "headlines_df = pd.DataFrame(data)\n",
    "print(headlines_df.head())\n",
    "```\n",
    "\n",
    "### Notes:\n",
    "- This is **for educational purposes only**. Sites may block or throttle bots.\n",
    "- Always refer to the site’s `robots.txt` (e.g., `https://www.cnbc.com/robots.txt`).\n",
    "- For production-grade scraping, consider tools like `Selenium`, `Scrapy`, or `Playwright`.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Page 4 will showcase exploratory data analysis techniques applied to scraped financial text data, including sentiment and keyword analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page 4: Exploratory Data Analysis of Web-Scraped Financial Text\n",
    "\n",
    "## Overview\n",
    "\n",
    "Once financial data has been scraped and structured, the next critical step is **Exploratory Data Analysis (EDA)**. EDA helps uncover patterns, spot anomalies, and identify trends within the data, allowing analysts to evaluate signal strength before feeding it into predictive models. For textual data such as headlines or social media posts, EDA often involves **natural language processing (NLP)** techniques like sentiment analysis, keyword frequency, and topic modeling.\n",
    "\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "Before performing EDA, the raw text needs to be cleaned and normalized:\n",
    "\n",
    "### a. Preprocessing Steps\n",
    "- **Lowercasing** all text\n",
    "- **Removing punctuation**, numbers, and stopwords\n",
    "- **Tokenization** (splitting text into words)\n",
    "- **Lemmatization or stemming** to reduce words to their base forms\n",
    "- **Handling duplicates** and missing values\n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned_headline'] = df['headline'].apply(preprocess)\n",
    "```\n",
    "\n",
    "\n",
    "## 2. Word Frequency and Visualization\n",
    "\n",
    "After cleaning the text, a simple yet insightful step is analyzing word frequency using a **word cloud** or bar plot:\n",
    "\n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a single text blob\n",
    "all_text = ' '.join(df['cleaned_headline'])\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Most Frequent Words in Financial Headlines\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Interpretation:\n",
    "Frequent keywords like “earnings”, “inflation”, “cut”, or “growth” might reflect macroeconomic concerns or company-specific trends.\n",
    "\n",
    "\n",
    "## 3. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis provides a proxy for public or media mood around market topics. We can use `TextBlob` or `VADER` to derive sentiment polarity:\n",
    "\n",
    "```python\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Compute polarity score between -1 (negative) to +1 (positive)\n",
    "df['sentiment'] = df['headline'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.hist(df['sentiment'], bins=20, edgecolor='black')\n",
    "plt.title(\"Distribution of Headline Sentiment\")\n",
    "plt.xlabel(\"Sentiment Polarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Insight:\n",
    "A skew toward positive sentiment may suggest optimism, while high variance in sentiment can reflect uncertainty or mixed market signals.\n",
    "\n",
    "## 4. Temporal Analysis (Optional)\n",
    "\n",
    "If timestamp data is available, you can analyze **sentiment or keyword spikes over time**:\n",
    "\n",
    "```python\n",
    "# Example: Daily average sentiment\n",
    "sentiment_by_date = df.groupby('date')['sentiment'].mean()\n",
    "sentiment_by_date.plot(figsize=(10,4), title=\"Daily Average Sentiment\")\n",
    "plt.ylabel(\"Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "Through EDA, analysts can transform unstructured text into actionable insights. High-frequency keywords reveal thematic concentrations, while sentiment scores can serve as predictive features. Although EDA is not sufficient for final decision-making, it is an essential **first step** in validating whether scraped data has potential alpha-generating properties.\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Page 5 will provide a short literature review of related work using web-scraped data in financial applications, along with references.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page 5: Literature Review and References\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "A robust body of academic and industry research supports the use of web-scraped data for financial analysis. This page provides a concise **literature review** highlighting key studies that have demonstrated the predictive power and strategic value of alternative web data—particularly in the form of news articles, social media sentiment, and financial forum discussions.\n",
    "The studies reviewed here focus on the application of web-scraped data in various financial contexts, including stock price prediction, sentiment analysis, and market trend detection. They illustrate the growing recognition of web-scraped data as a valuable asset class in quantitative finance.\n",
    "\n",
    "## 2. Selected Studies\n",
    "\n",
    "### a. Bollen et al. (2011) – Twitter Mood Predicts the Stock Market\n",
    "- **Key Contribution**: Introduced the concept of using aggregated Twitter sentiment (mood states) to predict the Dow Jones Industrial Average.\n",
    "- **Findings**: Daily sentiment trends on Twitter—especially calmness and anxiety—had predictive power, leading to **87.6% accuracy** in directional prediction.\n",
    "- **Significance**: This was one of the earliest large-scale validations of using social sentiment as a market signal.\n",
    "\n",
    "### b. Huang et al. (2018) – Sentiment and Asset Pricing\n",
    "- **Journal**: *Journal of Financial Economics*\n",
    "- **Key Contribution**: Examined how online investor sentiment influences stock returns.\n",
    "- **Findings**: Sentiment extracted from Internet forums offered information beyond fundamentals, particularly for hard-to-value or highly volatile stocks.\n",
    "- **Significance**: Empirical evidence supporting the inclusion of web-derived sentiment in pricing models.\n",
    "\n",
    "### c. Nassirtoussi et al. (2015) – Systematic Review on Text Mining for Market Prediction\n",
    "- **Scope**: Meta-analysis of 30+ studies using NLP for financial forecasting.\n",
    "- **Findings**: Concluded that news articles and social media, when properly preprocessed, can enhance market prediction models.\n",
    "- **Added Value**: Categorized methodologies (SVM, Decision Trees, Naive Bayes) and compared their performance.\n",
    "\n",
    "### d. Pagolu et al. (2016) – Sentiment Analysis for Stock Movement Prediction\n",
    "- **Conference**: International Conference on Computer Communication and Informatics (ICCCI)\n",
    "- **Key Contribution**: Used `TextBlob` and Twitter data to classify stock movement as up or down.\n",
    "- **Model Accuracy**: Achieved promising classification accuracy with minimal features.\n",
    "- **Significance**: Reinforced the use of simple NLP tools for retail investors.\n",
    "\n",
    "\n",
    "## 3. Emerging Trends in Web-Scraped Financial Data\n",
    "\n",
    "- **Topic Modeling**: Use of LDA and BERT-based embeddings to identify emerging financial themes.\n",
    "- **Multi-Modal Approaches**: Combining sentiment, volume, and price signals across different web platforms.\n",
    "- **Real-Time Alpha**: Integration of streaming web data into high-frequency trading pipelines.\n",
    "- **Event Detection**: Automated detection of exogenous shocks (e.g., bankruptcy filings, CEO scandals) before they hit mainstream news.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Research Implications\n",
    "\n",
    "These studies validate the **practical viability** of using web-scraped alternative data in asset management, trading, and market surveillance. They demonstrate that:\n",
    "- **Investor sentiment**—even from noisy platforms—can carry predictive content.\n",
    "- **NLP pipelines** (when tuned) provide robust signals for trend detection.\n",
    "- **Real-time scraping** and analysis can supplement or even replace some traditional data vendors.\n",
    "\n",
    "However, successful application requires attention to **data quality**, **bias mitigation**, and **ethical use**, as discussed in earlier sections.\n",
    "\n",
    "\n",
    "## 5. References (MLA Format)\n",
    "\n",
    "- Bollen, Johan, Huina Mao, and Xiaojun Zeng. “Twitter Mood Predicts the Stock Market.” *Journal of Computational Science*, vol. 2, no. 1, 2011, pp. 1–8.\n",
    "- Huang, Alan, Annie Zang, and Rong Zheng. “Evidence from the Internet: Investors’ Sentiment and Stock Returns.” *Journal of Financial Economics*, vol. 129, no. 2, 2018, pp. 315–338.\n",
    "- Nassirtoussi, A.K., et al. “Text Mining for Market Prediction: A Systematic Review.” *Expert Systems with Applications*, vol. 42, no. 1, 2015, pp. 613–632.\n",
    "- Pagolu, Venkata Subrahmanyam, et al. “Sentiment Analysis of Twitter Data for Predicting Stock Market Movements.” *2016 International Conference on Computer Communication and Informatics (ICCCI)*, IEEE, 2016.\n",
    "\n",
    "---\n",
    "\n",
    "*This completes the 5-page user guide on evaluating web scraping as a category of alternative data for financial applications.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
